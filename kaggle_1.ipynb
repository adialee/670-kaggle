{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c6de69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d84697f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in datasets\n",
    "training_data = pd.read_csv(\"si670_kaggle1_train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "validation_data = pd.read_csv(\"si670_kaggle1_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "27144859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and labels\n",
    "X_train, y_train = training_data[\"text\"], training_data[\"label\"]\n",
    "X_val, y_val = validation_data[\"text\"], validation_data[\"label\"]\n",
    "X_test = test_data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ac3d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 19\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# pipe = Pipeline([\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     ('vect', TfidfVectorizer()),\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#     }\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m---> 19\u001b[0m X_train_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m X_val_tfidf \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_val)\n\u001b[0;32m     21\u001b[0m X_test_tfidf \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test)\n",
      "File \u001b[1;32mc:\\Users\\adia3\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2099\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2100\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2101\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2102\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2103\u001b[0m )\n\u001b[1;32m-> 2104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adia3\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adia3\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adia3\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1262\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1264\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1265\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "# pipe = Pipeline([\n",
    "#     ('vect', TfidfVectorizer()),\n",
    "#     ('logreg', LogisticRegression())\n",
    "# ])\n",
    "\n",
    "# param_grid = [\n",
    "#     {\n",
    "#         'vect': [CountVectorizer(), TfidfVectorizer()],\n",
    "#         'vect__ngram_range': [(1,1), (1,2)],\n",
    "#         'vect__min_df': [1, 2],\n",
    "#         'logreg__C': [0.01, 0.1, 1, 10],\n",
    "#         'logreg__penalty': ['l2'],\n",
    "#         'logreg__solver': ['lbfgs']\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lg = LogisticRegression(penalty='l1',solver='liblinear')\n",
    "# sv = SVC(kernel='linear',gamma=1.0)\n",
    "# mnb = MultinomialNB()\n",
    "# dtc = DecisionTreeClassifier(max_depth=5)\n",
    "# knn = KNeighborsClassifier(n_neighbors=5)\n",
    "# rfc = RandomForestClassifier(n_estimators=50,random_state=42)\n",
    "# etc = ExtraTreesClassifier(n_estimators=50,random_state=42)\n",
    "# abc = AdaBoostClassifier(n_estimators=50,random_state=42)\n",
    "# bg = BaggingClassifier(n_estimators=50,random_state=42)\n",
    "# gbc = GradientBoostingClassifier(n_estimators=50,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70033ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = {}\n",
    "f1_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a09e3fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7426926327651782\n",
      "0.7504994109511858\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# pipeline = Pipeline([\n",
    "#     (\"tfidf\", TfidfVectorizer(max_features=50000, ngram_range=(1,2))),\n",
    "#     # (\"svd\", TruncatedSVD(n_components=100, random_state=42)),\n",
    "#     (\"lin_svc\", LinearSVC(C=c, random_state=42))\n",
    "# ])\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=50000, ngram_range=(1,2), analyzer='word',\n",
    "                              min_df=2, max_df=0.9)),\n",
    "    # (\"svd\", TruncatedSVD(n_components=100, random_state=42)),\n",
    "    (\"lin_svc\", LinearSVC(C=1, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_val)\n",
    "    # accuracy_scores[c] = accuracy_score(y_val, predictions)\n",
    "    # f1_scores[c] = f1_score(y_val, predictions)\n",
    "\n",
    "print(accuracy_score(y_val, predictions))\n",
    "print(f1_score(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "241bb88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7300676151570644, F1 Score: 0.711722892925646\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_val, predictions)\n",
    "f1 = f1_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy}, F1 Score: {f1}\")\n",
    "accuracy_scores[\"log_reg\"] = accuracy\n",
    "f1_scores[\"log_reg\"] = f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b7d9b",
   "metadata": {},
   "source": [
    "Accuracy: 0.7300676151570644, F1 Score: 0.711722892925646\n",
    "\n",
    "`pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=50000, ngram_range=(1,2), analyzer='word',\n",
    "                              min_df=2, max_df=0.9)),\n",
    "    # (\"svd\", TruncatedSVD(n_components=100, random_state=42)),\n",
    "    (\"lin_svc\", LinearSVC(C=1, random_state=42))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09afafe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# etc.fit(X_train_tfidf, y_train)\n",
    "# pred = etc.predict(X_val_tfidf)\n",
    "# accuracy = accuracy_score(y_val, pred)\n",
    "# f1 = f1_score(y_val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1086ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"si670_kaggle1_predictions.csv\", \"w\") as f:\n",
    "    f.write(\"id,label\\n\")\n",
    "    for i in range(len(final_predictions)):\n",
    "        f.write(f\"{X_test.index[i]},{final_predictions[i]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a8caabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prediction(model,X_train,X_val,y_train,y_val):\n",
    "#     model.fit(X_train,y_train)\n",
    "#     pred = model.predict(X_val)\n",
    "#     acc_score = accuracy_score(y_val,pred)\n",
    "#     f1 = f1_score(y_val, pred)\n",
    "#     return acc_score,f1\n",
    "\n",
    "# acc_scores = {}\n",
    "# f1_scores = {}\n",
    "\n",
    "# clf = knn\n",
    "# acc_scores[\"knn\"],f1_scores[\"knn\"]= prediction(clf,X_train_tfidf,X_val_tfidf,y_train,y_val)\n",
    "# print(f\"Accuracy: {acc_scores['knn']}, F1 Score: {f1_scores['knn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bebcfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prediction(model,X_train,X_val,y_train,y_val):\n",
    "#     model.fit(X_train,y_train)\n",
    "#     pred = model.predict(X_val)\n",
    "#     acc_score = accuracy_score(y_val,pred)\n",
    "#     f1 = f1_score(y_val, pred)\n",
    "#     return acc_score,f1\n",
    "   \n",
    "# acc_scores = {}\n",
    "# f1_scores = {}\n",
    "# clfs= {\n",
    "#     'LR':lg,\n",
    "#     'SVM':sv,\n",
    "#     # 'DTC':dtc,\n",
    "#     'KNN':knn,\n",
    "#     # 'RFC':rfc,\n",
    "#     # 'ETC':etc,\n",
    "#     # 'ABC':abc,\n",
    "#     # 'BG':bg,\n",
    "#     # 'GBC':gbc,\n",
    "# }\n",
    "# for name,clf in clfs.items():\n",
    "#     acc_scores[name],f1_scores[name]= prediction(clf,X_train_tfidf,X_val_tfidf,y_train,y_val)\n",
    "    \n",
    "#     #View those scores\n",
    "#     acc_scores\n",
    "#     f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f23926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = GridSearchCV(pipe, param_grid, cv=3, scoring=\"f1\", n_jobs=-1)\n",
    "\n",
    "# # TODO: Fit the grid search on the training data\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# # TODO: Print the best parameters and CV score\n",
    "# print(\"Best parameters:\", grid.best_params_)\n",
    "# print(\"Best CV score:\", grid.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
